{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "from kan import *\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from treasury_base import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LIST = [5, 10]\n",
    "LAG = 1\n",
    "\n",
    "def train_mse():\n",
    "    predictions = model(dataset['train_input'])  # Model predictions\n",
    "    mse = F.mse_loss(predictions, dataset['train_label'], reduction='mean')  # Compute MSE\n",
    "    return mse ** 0.5  # Return scalar MSE value\n",
    "\n",
    "def test_mse():\n",
    "    predictions = model(dataset['test_input']) # Model predictions\n",
    "    mse = F.mse_loss(predictions, dataset['test_label'], reduction='mean')  # Compute MSE\n",
    "    return mse ** 0.5\n",
    "    \n",
    "df_ma = ma_data_retrieval(window_list=WINDOW_LIST, lag=LAG)\n",
    "df_ma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1\n",
    "LENGTH = len(df_ma)\n",
    "TARGETS = df_ma.columns[:12]\n",
    "\n",
    "# Store results for each fold\n",
    "fold_results = {'train_mse': [], 'test_mse': [], 'naive_mse': []}\n",
    "\n",
    "for cnt in range(0, 20, 5):\n",
    "    print()\n",
    "    print('WINDOW SLIDING: ', cnt)\n",
    "\n",
    "    df_window = df_ma[(LENGTH-cnt-250):(LENGTH-cnt)]\n",
    "    # Prepare data\n",
    "    X, y = df_window.drop(columns=TARGETS), df_window[TARGETS]\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "    n_inputs = X.shape[1]\n",
    "    n_outputs = y.shape[1]\n",
    "\n",
    "    X_train, X_test = X[:-TEST_SIZE], X[-TEST_SIZE:]\n",
    "    y_train, y_test = y[:-TEST_SIZE], y[-TEST_SIZE:]\n",
    "\n",
    "    dataset = dict()\n",
    "    dtype = torch.get_default_dtype()\n",
    "    dataset['train_input'] = torch.from_numpy(X_train.values).type(dtype).to(device)\n",
    "    dataset['train_label'] = torch.from_numpy(y_train.values).type(dtype).to(device)\n",
    "    dataset['test_input'] = torch.from_numpy(X_test.values).type(dtype).to(device)\n",
    "    dataset['test_label'] = torch.from_numpy(y_test.values).type(dtype).to(device)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = KAN(width=[n_inputs, 48, 64, n_outputs], grid=4, k=2, seed=42, device=device)\n",
    "\n",
    "    # Train the model and compute metrics\n",
    "    results = model.fit(dataset, opt=\"Adam\", lamb=0.0001, lr=0.001, steps=500, metrics=(train_mse, test_mse))\n",
    "    df_naive = pd.DataFrame([y_train.iloc[-1]] * TEST_SIZE, columns=y_train.columns)\n",
    "        \n",
    "    # Store the metrics\n",
    "    train_error = results['train_mse'][-1]\n",
    "    test_error = results['test_mse'][-1]\n",
    "    naive_error = mean_squared_error(df_naive, y_test, squared=False)\n",
    "\n",
    "    fold_results['train_mse'].append(train_error)\n",
    "    fold_results['test_mse'].append(test_error)\n",
    "    fold_results['naive_mse'].append(naive_error)\n",
    "\n",
    "    # Calculate average metrics across all windows\n",
    "    print(f'Fold Train MSE: {train_error}')\n",
    "    print(f'Fold Test MSE: {test_error}')\n",
    "    print(f'Naive Test MSE: {naive_error}')\n",
    "\n",
    "avg_train_mse = np.mean(fold_results['train_mse'])\n",
    "avg_test_mse = np.mean(fold_results['test_mse'])\n",
    "avg_naive_mse = np.mean(fold_results['naive_mse'])\n",
    "\n",
    "print()\n",
    "print(\"Sliding Window Cross-Validation Results\")\n",
    "print(f\"Average Train MSE: {avg_train_mse}\")\n",
    "print(f\"Average Test MSE: {avg_test_mse}\")\n",
    "print(f\"Average Naive MSE: {avg_naive_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WINDOW SLIDING: 0, LAG: 1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.74e+00 | test_loss: 3.54e+00 | reg: 2.19e+02 | : 100%|█| 10/10 [00:01<00:00,  7.88it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 3.6459810856273243\n",
      "Fold Test MSE: 3.5396966201798845\n",
      "Naive Test MSE: 0.031666666666666655\n",
      "\n",
      "WINDOW SLIDING: 1, LAG: 1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.75e+00 | test_loss: 3.57e+00 | reg: 2.18e+02 | : 100%|█| 10/10 [00:00<00:00, 11.91it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 3.651486329433173\n",
      "Fold Test MSE: 3.5667371309929905\n",
      "Naive Test MSE: 0.01500000000000005\n",
      "\n",
      "Sliding Window Cross-Validation Results\n",
      "Average Train MSE: 3.6487337075302486\n",
      "Average Test MSE: 3.5532168755864375\n",
      "Average Naive MSE: 0.023333333333333352\n"
     ]
    }
   ],
   "source": [
    "WINDOW_LIST = [5, 10]\n",
    "TEST_SIZE = 1\n",
    "LENGTH = len(df_ma)\n",
    "TARGETS = df_ma.columns[:12]\n",
    "\n",
    "# Store results for each fold\n",
    "fold_results = {'train_mse': [], 'test_mse': [], 'naive_mse': []}\n",
    "\n",
    "for LAG in range(1, 2):\n",
    "    df_ma = ma_data_retrieval(window_list=WINDOW_LIST, lag=LAG)\n",
    "\n",
    "    for cnt in range(0, 2):\n",
    "        print()\n",
    "        print(f'WINDOW SLIDING: {cnt}, LAG: {LAG}')\n",
    "\n",
    "        df_window = df_ma[(LENGTH-cnt-100):(LENGTH-cnt)]\n",
    "        # Prepare data\n",
    "        X, y = df_window.drop(columns=TARGETS), df_window[TARGETS]\n",
    "\n",
    "        # scaler = StandardScaler()\n",
    "        # X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "        n_inputs = X.shape[1]\n",
    "        n_outputs = y.shape[1]\n",
    "\n",
    "        X_train, X_test = X[:-TEST_SIZE], X[-TEST_SIZE:]\n",
    "        y_train, y_test = y[:-TEST_SIZE], y[-TEST_SIZE:]\n",
    "\n",
    "        dataset = dict()\n",
    "        dtype = torch.get_default_dtype()\n",
    "        dataset['train_input'] = torch.from_numpy(X_train.values).type(dtype).to(device)\n",
    "        dataset['train_label'] = torch.from_numpy(y_train.values).type(dtype).to(device)\n",
    "        dataset['test_input'] = torch.from_numpy(X_test.values).type(dtype).to(device)\n",
    "        dataset['test_label'] = torch.from_numpy(y_test.values).type(dtype).to(device)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = KAN(width=[n_inputs, 48, 64, n_outputs], grid=4, k=2, seed=42, device=device)\n",
    "\n",
    "        # Train the model and compute metrics\n",
    "        results = model.fit(dataset, opt=\"Adam\", lamb=0.0001, lr=0.001, steps=10, metrics=(train_mse, test_mse))\n",
    "        df_naive = pd.DataFrame([y_train.iloc[-LAG]] * TEST_SIZE, columns=y_train.columns)\n",
    "            \n",
    "        # Store the metrics\n",
    "        train_error = results['train_mse'][-1]\n",
    "        test_error = results['test_mse'][-1]\n",
    "        naive_error = mean_squared_error(df_naive, y_test, squared=False)\n",
    "\n",
    "        fold_results['train_mse'].append(train_error)\n",
    "        fold_results['test_mse'].append(test_error)\n",
    "        fold_results['naive_mse'].append(naive_error)\n",
    "\n",
    "        # Calculate average metrics across all windows\n",
    "        print(f'Fold Train MSE: {train_error}')\n",
    "        print(f'Fold Test MSE: {test_error}')\n",
    "        print(f'Naive Test MSE: {naive_error}')\n",
    "\n",
    "avg_train_mse = np.mean(fold_results['train_mse'])\n",
    "avg_test_mse = np.mean(fold_results['test_mse'])\n",
    "avg_naive_mse = np.mean(fold_results['naive_mse'])\n",
    "\n",
    "print()\n",
    "print(\"Sliding Window Cross-Validation Results\")\n",
    "print(f\"Average Train MSE: {avg_train_mse}\")\n",
    "print(f\"Average Test MSE: {avg_test_mse}\")\n",
    "print(f\"Average Naive MSE: {avg_naive_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float('x', -10, 10)\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "study.best_params  # E.g. {'x': 2.002108042}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "\n",
    "def train_mse(model, dataset):\n",
    "    predictions = model(dataset['train_input'])  # Model predictions\n",
    "    loss = torch.nn.functional.mse_loss(predictions, dataset['train_label'])\n",
    "    return loss\n",
    "\n",
    "def test_mse(model, dataset):\n",
    "    predictions = model(dataset['test_input'])  # Model predictions\n",
    "    loss = torch.nn.functional.mse_loss(predictions, dataset['test_label'])\n",
    "    return loss\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 2)  # Number of layers in the network\n",
    "    layer_sizes = [trial.suggest_int(f'n_units_l{i}', 16, 64, step=16) for i in range(n_layers)]\n",
    "    grid = trial.suggest_int('grid', 2, 4)          # Example parameter for KAN\n",
    "    lamb = trial.suggest_float('lamb', 1e-4, 1e-2, log=True)  # Regularization rate\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)       # Learning rate\n",
    "    steps = trial.suggest_int('steps', 500, 2000, step=500)   # Training steps\n",
    "\n",
    "    # Model architecture\n",
    "    width = [n_inputs] + layer_sizes + [n_outputs]\n",
    "\n",
    "    # Initialize dataset\n",
    "    dataset = dict()\n",
    "    dtype = torch.get_default_dtype()\n",
    "    dataset['train_input'] = torch.from_numpy(X_train.values).type(dtype).to(device)\n",
    "    dataset['train_label'] = torch.from_numpy(y_train.values).type(dtype).to(device)\n",
    "    dataset['test_input'] = torch.from_numpy(X_test.values).type(dtype).to(device)\n",
    "    dataset['test_label'] = torch.from_numpy(y_test.values).type(dtype).to(device)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = KAN(width=width, grid=grid, k=2, seed=42, device=device)\n",
    "\n",
    "    # Train the model\n",
    "    results = model.fit(\n",
    "        dataset, \n",
    "        opt=\"Adam\", \n",
    "        lamb=lamb, \n",
    "        lr=lr, \n",
    "        steps=steps, \n",
    "        metrics=(lambda: train_mse(model, dataset), lambda: test_mse(model, dataset))\n",
    "    )\n",
    "\n",
    "    # Retrieve the metric (e.g., test MSE) from the results\n",
    "    test_mse_value = results['test_loss'][-1]\n",
    "    return test_mse_value  # Minimize test MSE\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best parameters and results\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best test MSE:\", study.best_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
