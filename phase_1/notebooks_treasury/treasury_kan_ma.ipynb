{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "from kan import *\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from treasury_base import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 Mo</th>\n",
       "      <th>2 Mo</th>\n",
       "      <th>3 Mo</th>\n",
       "      <th>6 Mo</th>\n",
       "      <th>1 Yr</th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>3 Yr</th>\n",
       "      <th>5 Yr</th>\n",
       "      <th>7 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "      <th>...</th>\n",
       "      <th>10 Yr_MA10</th>\n",
       "      <th>10 Yr_MA20</th>\n",
       "      <th>20 Yr_MA3</th>\n",
       "      <th>20 Yr_MA5</th>\n",
       "      <th>20 Yr_MA10</th>\n",
       "      <th>20 Yr_MA20</th>\n",
       "      <th>30 Yr_MA3</th>\n",
       "      <th>30 Yr_MA5</th>\n",
       "      <th>30 Yr_MA10</th>\n",
       "      <th>30 Yr_MA20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>2.42</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.63</td>\n",
       "      <td>...</td>\n",
       "      <td>2.742</td>\n",
       "      <td>2.7180</td>\n",
       "      <td>2.906667</td>\n",
       "      <td>2.906</td>\n",
       "      <td>2.917</td>\n",
       "      <td>2.8935</td>\n",
       "      <td>3.053333</td>\n",
       "      <td>3.052</td>\n",
       "      <td>3.062</td>\n",
       "      <td>3.0375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.70</td>\n",
       "      <td>...</td>\n",
       "      <td>2.732</td>\n",
       "      <td>2.7165</td>\n",
       "      <td>2.876667</td>\n",
       "      <td>2.894</td>\n",
       "      <td>2.908</td>\n",
       "      <td>2.8935</td>\n",
       "      <td>3.030000</td>\n",
       "      <td>3.042</td>\n",
       "      <td>3.054</td>\n",
       "      <td>3.0385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-04</th>\n",
       "      <td>2.41</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.73</td>\n",
       "      <td>...</td>\n",
       "      <td>2.727</td>\n",
       "      <td>2.7235</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>2.886</td>\n",
       "      <td>2.903</td>\n",
       "      <td>2.9000</td>\n",
       "      <td>3.026667</td>\n",
       "      <td>3.036</td>\n",
       "      <td>3.050</td>\n",
       "      <td>3.0440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-05</th>\n",
       "      <td>2.39</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>2.721</td>\n",
       "      <td>2.7265</td>\n",
       "      <td>2.876667</td>\n",
       "      <td>2.886</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.9045</td>\n",
       "      <td>3.026667</td>\n",
       "      <td>3.036</td>\n",
       "      <td>3.047</td>\n",
       "      <td>3.0480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-06</th>\n",
       "      <td>2.40</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.70</td>\n",
       "      <td>...</td>\n",
       "      <td>2.718</td>\n",
       "      <td>2.7270</td>\n",
       "      <td>2.896667</td>\n",
       "      <td>2.884</td>\n",
       "      <td>2.898</td>\n",
       "      <td>2.9060</td>\n",
       "      <td>3.040000</td>\n",
       "      <td>3.034</td>\n",
       "      <td>3.044</td>\n",
       "      <td>3.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1 Mo  2 Mo  3 Mo  6 Mo  1 Yr  2 Yr  3 Yr  5 Yr  7 Yr  10 Yr  ...  \\\n",
       "Date                                                                     ...   \n",
       "2019-01-31  2.42  2.43  2.41  2.46  2.55  2.45  2.43  2.43  2.51   2.63  ...   \n",
       "2019-02-01  2.41  2.42  2.40  2.46  2.56  2.52  2.50  2.51  2.59   2.70  ...   \n",
       "2019-02-04  2.41  2.41  2.42  2.49  2.57  2.53  2.52  2.53  2.62   2.73  ...   \n",
       "2019-02-05  2.39  2.40  2.42  2.50  2.56  2.53  2.50  2.51  2.60   2.71  ...   \n",
       "2019-02-06  2.40  2.41  2.42  2.50  2.56  2.52  2.50  2.50  2.59   2.70  ...   \n",
       "\n",
       "            10 Yr_MA10  10 Yr_MA20  20 Yr_MA3  20 Yr_MA5  20 Yr_MA10  \\\n",
       "Date                                                                   \n",
       "2019-01-31       2.742      2.7180   2.906667      2.906       2.917   \n",
       "2019-02-01       2.732      2.7165   2.876667      2.894       2.908   \n",
       "2019-02-04       2.727      2.7235   2.870000      2.886       2.903   \n",
       "2019-02-05       2.721      2.7265   2.876667      2.886       2.900   \n",
       "2019-02-06       2.718      2.7270   2.896667      2.884       2.898   \n",
       "\n",
       "            20 Yr_MA20  30 Yr_MA3  30 Yr_MA5  30 Yr_MA10  30 Yr_MA20  \n",
       "Date                                                                  \n",
       "2019-01-31      2.8935   3.053333      3.052       3.062      3.0375  \n",
       "2019-02-01      2.8935   3.030000      3.042       3.054      3.0385  \n",
       "2019-02-04      2.9000   3.026667      3.036       3.050      3.0440  \n",
       "2019-02-05      2.9045   3.026667      3.036       3.047      3.0480  \n",
       "2019-02-06      2.9060   3.040000      3.034       3.044      3.0500  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WINDOW_LIST = [3, 5, 10, 20]\n",
    "LAG = 1\n",
    "\n",
    "def train_mse():\n",
    "    predictions = model(dataset['train_input'])  # Model predictions\n",
    "    mse = F.mse_loss(predictions, dataset['train_label'], reduction='mean')  # Compute MSE\n",
    "    return mse ** 0.5  # Return scalar MSE value\n",
    "\n",
    "def test_mse():\n",
    "    predictions = model(dataset['test_input']) # Model predictions\n",
    "    mse = F.mse_loss(predictions, dataset['test_label'], reduction='mean')  # Compute MSE\n",
    "    return mse ** 0.5\n",
    "    \n",
    "df_ma = ma_data_retrieval(window_list=WINDOW_LIST, lag=LAG)\n",
    "df_ma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1\n",
    "LENGTH = len(df_ma)\n",
    "TARGETS = df_ma.columns[:12]\n",
    "\n",
    "# Store results for each fold\n",
    "fold_results = {'train_mse': [], 'test_mse': [], 'naive_mse': []}\n",
    "\n",
    "for cnt in range(0, 20, 5):\n",
    "    print()\n",
    "    print('WINDOW SLIDING: ', cnt)\n",
    "\n",
    "    df_window = df_ma[(LENGTH-cnt-250):(LENGTH-cnt)]\n",
    "    # Prepare data\n",
    "    X, y = df_window.drop(columns=TARGETS), df_window[TARGETS]\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "    n_inputs = X.shape[1]\n",
    "    n_outputs = y.shape[1]\n",
    "\n",
    "    X_train, X_test = X[:-TEST_SIZE], X[-TEST_SIZE:]\n",
    "    y_train, y_test = y[:-TEST_SIZE], y[-TEST_SIZE:]\n",
    "\n",
    "    dataset = dict()\n",
    "    dtype = torch.get_default_dtype()\n",
    "    dataset['train_input'] = torch.from_numpy(X_train.values).type(dtype).to(device)\n",
    "    dataset['train_label'] = torch.from_numpy(y_train.values).type(dtype).to(device)\n",
    "    dataset['test_input'] = torch.from_numpy(X_test.values).type(dtype).to(device)\n",
    "    dataset['test_label'] = torch.from_numpy(y_test.values).type(dtype).to(device)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = KAN(width=[n_inputs, 48, 64, n_outputs], grid=4, k=2, seed=42, device=device)\n",
    "\n",
    "    # Train the model and compute metrics\n",
    "    results = model.fit(dataset, opt=\"Adam\", lamb=0.0001, lr=0.001, steps=500, metrics=(train_mse, test_mse))\n",
    "    df_naive = pd.DataFrame([y_train.iloc[-1]] * TEST_SIZE, columns=y_train.columns)\n",
    "        \n",
    "    # Store the metrics\n",
    "    train_error = results['train_mse'][-1]\n",
    "    test_error = results['test_mse'][-1]\n",
    "    naive_error = mean_squared_error(df_naive, y_test, squared=False)\n",
    "\n",
    "    fold_results['train_mse'].append(train_error)\n",
    "    fold_results['test_mse'].append(test_error)\n",
    "    fold_results['naive_mse'].append(naive_error)\n",
    "\n",
    "    # Calculate average metrics across all windows\n",
    "    print(f'Fold Train MSE: {train_error}')\n",
    "    print(f'Fold Test MSE: {test_error}')\n",
    "    print(f'Naive Test MSE: {naive_error}')\n",
    "\n",
    "avg_train_mse = np.mean(fold_results['train_mse'])\n",
    "avg_test_mse = np.mean(fold_results['test_mse'])\n",
    "avg_naive_mse = np.mean(fold_results['naive_mse'])\n",
    "\n",
    "print()\n",
    "print(\"Sliding Window Cross-Validation Results\")\n",
    "print(f\"Average Train MSE: {avg_train_mse}\")\n",
    "print(f\"Average Test MSE: {avg_test_mse}\")\n",
    "print(f\"Average Naive MSE: {avg_naive_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WINDOW SLIDING: 0, LAG: 1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.27e-02 | test_loss: 1.19e-01 | reg: 1.16e+02 | : 100%|█| 750/750 [01:21<00:00,  9.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07256363261841549\n",
      "Fold Test MSE: 0.11935505068443238\n",
      "Naive Test MSE: 0.11094668389215898\n",
      "\n",
      "WINDOW SLIDING: 10, LAG: 1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.34e-02 | test_loss: 2.00e-01 | reg: 1.15e+02 | : 100%|█| 750/750 [01:23<00:00,  9.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07320320509691446\n",
      "Fold Test MSE: 0.2001554887713319\n",
      "Naive Test MSE: 0.12736921396737383\n",
      "\n",
      "WINDOW SLIDING: 20, LAG: 1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.50e-02 | test_loss: 1.77e-01 | reg: 1.20e+02 | : 100%|█| 750/750 [01:19<00:00,  9.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07493622462960524\n",
      "Fold Test MSE: 0.1767447413707237\n",
      "Naive Test MSE: 0.14997221964972937\n",
      "\n",
      "WINDOW SLIDING: 30, LAG: 1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.56e-02 | test_loss: 2.09e-01 | reg: 1.20e+02 | : 100%|█| 750/750 [01:18<00:00,  9.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07561153172117846\n",
      "Fold Test MSE: 0.20884070705014865\n",
      "Naive Test MSE: 0.26592213772706724\n",
      "\n",
      "WINDOW SLIDING: 0, LAG: 2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.84e-02 | test_loss: 1.26e-01 | reg: 1.18e+02 | : 100%|█| 750/750 [01:18<00:00,  9.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07876269390699238\n",
      "Fold Test MSE: 0.1256567377237061\n",
      "Naive Test MSE: 0.09790726905257505\n",
      "\n",
      "WINDOW SLIDING: 10, LAG: 2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.97e-02 | test_loss: 2.11e-01 | reg: 1.17e+02 | : 100%|█| 750/750 [01:18<00:00,  9.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07959999060822966\n",
      "Fold Test MSE: 0.21064221537398026\n",
      "Naive Test MSE: 0.15235785725280676\n",
      "\n",
      "WINDOW SLIDING: 20, LAG: 2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.21e-02 | test_loss: 2.02e-01 | reg: 1.21e+02 | : 100%|█| 750/750 [01:19<00:00,  9.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08204245117727815\n",
      "Fold Test MSE: 0.2017624642923682\n",
      "Naive Test MSE: 0.156276997667603\n",
      "\n",
      "WINDOW SLIDING: 30, LAG: 2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.30e-02 | test_loss: 2.31e-01 | reg: 1.22e+02 | : 100%|█| 750/750 [01:19<00:00,  9.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08298026063768746\n",
      "Fold Test MSE: 0.23130260539096742\n",
      "Naive Test MSE: 0.24277647469774882\n",
      "\n",
      "WINDOW SLIDING: 0, LAG: 3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.93e-02 | test_loss: 1.22e-01 | reg: 1.21e+02 | : 100%|█| 750/750 [01:20<00:00,  9.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07959389667048104\n",
      "Fold Test MSE: 0.1222689244265049\n",
      "Naive Test MSE: 0.10017068766194369\n",
      "\n",
      "WINDOW SLIDING: 10, LAG: 3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.26e-02 | test_loss: 2.00e-01 | reg: 1.20e+02 | : 100%|█| 750/750 [01:20<00:00,  9.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08264071911118447\n",
      "Fold Test MSE: 0.20008421091520867\n",
      "Naive Test MSE: 0.16670707843400048\n",
      "\n",
      "WINDOW SLIDING: 20, LAG: 3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.65e-02 | test_loss: 2.24e-01 | reg: 1.21e+02 | : 100%|█| 750/750 [01:20<00:00,  9.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08641593899672162\n",
      "Fold Test MSE: 0.2235664177510921\n",
      "Naive Test MSE: 0.18431856480922734\n",
      "\n",
      "WINDOW SLIDING: 30, LAG: 3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.50e-02 | test_loss: 2.80e-01 | reg: 1.23e+02 | : 100%|█| 750/750 [01:18<00:00,  9.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08487054103439665\n",
      "Fold Test MSE: 0.2799849784960966\n",
      "Naive Test MSE: 0.2550187901573791\n",
      "\n",
      "WINDOW SLIDING: 0, LAG: 4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.08e-02 | test_loss: 1.30e-01 | reg: 1.21e+02 | : 100%|█| 750/750 [01:18<00:00,  9.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08083823213505514\n",
      "Fold Test MSE: 0.12973704182167167\n",
      "Naive Test MSE: 0.09919677414109794\n",
      "\n",
      "WINDOW SLIDING: 10, LAG: 4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.10e-02 | test_loss: 2.39e-01 | reg: 1.23e+02 | : 100%|█| 750/750 [01:19<00:00,  9.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08075559279961135\n",
      "Fold Test MSE: 0.23888998925420454\n",
      "Naive Test MSE: 0.2294676810940195\n",
      "\n",
      "WINDOW SLIDING: 20, LAG: 4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.88e-02 | test_loss: 2.28e-01 | reg: 1.22e+02 | : 100%|█| 750/750 [01:19<00:00,  9.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08876650676045206\n",
      "Fold Test MSE: 0.22781008200546632\n",
      "Naive Test MSE: 0.29922956627534875\n",
      "\n",
      "WINDOW SLIDING: 30, LAG: 4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.68e-02 | test_loss: 2.73e-01 | reg: 1.23e+02 | : 100%|█| 750/750 [01:19<00:00,  9.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.0866625856752807\n",
      "Fold Test MSE: 0.2729952600881827\n",
      "Naive Test MSE: 0.24907913735731993\n",
      "\n",
      "WINDOW SLIDING: 0, LAG: 5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.75e-02 | test_loss: 1.48e-01 | reg: 1.23e+02 | : 100%|█| 750/750 [01:20<00:00,  9.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07743177785799693\n",
      "Fold Test MSE: 0.1478257142420265\n",
      "Naive Test MSE: 0.10156278846112872\n",
      "\n",
      "WINDOW SLIDING: 10, LAG: 5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 7.78e-02 | test_loss: 2.50e-01 | reg: 1.26e+02 | : 100%|█| 750/750 [01:19<00:00,  9.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.07771074430551927\n",
      "Fold Test MSE: 0.25033751094265566\n",
      "Naive Test MSE: 0.2195630129750151\n",
      "\n",
      "WINDOW SLIDING: 20, LAG: 5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.63e-02 | test_loss: 1.87e-01 | reg: 1.25e+02 | : 100%|█| 750/750 [01:19<00:00,  9.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.08629425919955788\n",
      "Fold Test MSE: 0.186722485442877\n",
      "Naive Test MSE: 0.34475836948989846\n",
      "\n",
      "WINDOW SLIDING: 30, LAG: 5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 8.67e-02 | test_loss: 3.03e-01 | reg: 1.25e+02 | : 100%|█| 750/750 [01:20<00:00,  9.35"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Fold Train MSE: 0.0863870247650075\n",
      "Fold Test MSE: 0.3034451026913566\n",
      "Naive Test MSE: 0.2574805817921032\n",
      "\n",
      "Sliding Window Cross-Validation Results\n",
      "Average Train MSE: 0.08090339048537831\n",
      "Average Test MSE: 0.2029063864367501\n",
      "Average Naive MSE: 0.19050409432777726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "WINDOW_LIST = [3, 5, 10, 15, 20]\n",
    "TEST_SIZE = 20\n",
    "TARGETS = df_ma.columns[:12]\n",
    "\n",
    "# Store results for each fold\n",
    "fold_results = {'train_mse': [], 'test_mse': [], 'naive_mse': []}\n",
    "\n",
    "for LAG in range(1, 6): # steps into the future\n",
    "    df_ma = ma_data_retrieval(window_list=WINDOW_LIST, lag=LAG)\n",
    "\n",
    "    for cnt in range(0, 40, 10): # sliding window\n",
    "        print()\n",
    "        print(f'WINDOW SLIDING: {cnt}, LAG: {LAG}')\n",
    "\n",
    "        df_window = df_ma[(len(df_ma)-cnt-500):(len(df_ma)-cnt)]\n",
    "        # Prepare data\n",
    "        X, y = df_window.drop(columns=TARGETS), df_window[TARGETS]\n",
    "\n",
    "        # scaler = StandardScaler()\n",
    "        # X = pd.DataFrame(scaler.fit_transform(X))\n",
    "\n",
    "        n_inputs = X.shape[1]\n",
    "        n_outputs = y.shape[1]\n",
    "\n",
    "        X_train, X_test = X[:-TEST_SIZE], X[-TEST_SIZE:]\n",
    "        y_train, y_test = y[:-TEST_SIZE], y[-TEST_SIZE:]\n",
    "\n",
    "        dataset = dict()\n",
    "        dtype = torch.get_default_dtype()\n",
    "        dataset['train_input'] = torch.from_numpy(X_train.values).type(dtype).to(device)\n",
    "        dataset['train_label'] = torch.from_numpy(y_train.values).type(dtype).to(device)\n",
    "        dataset['test_input'] = torch.from_numpy(X_test.values).type(dtype).to(device)\n",
    "        dataset['test_label'] = torch.from_numpy(y_test.values).type(dtype).to(device)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = KAN(width=[n_inputs, 32, n_outputs], grid=4, k=2, seed=42, device=device)\n",
    "\n",
    "        # Train the model and compute metrics\n",
    "        results = model.fit(dataset, opt=\"Adam\", lamb=0.0001, lr=0.0015, steps=500, metrics=(train_mse, test_mse))\n",
    "        df_naive = pd.DataFrame([y_train.iloc[-LAG]] * TEST_SIZE, columns=y_train.columns)\n",
    "            \n",
    "        # Store the metrics\n",
    "        train_error = results['train_mse'][-1]\n",
    "        test_error = results['test_mse'][-1]\n",
    "        naive_error = mean_squared_error(df_naive.values.flatten(), y_test.values.flatten(), squared=False)\n",
    "\n",
    "        fold_results['train_mse'].append(train_error)\n",
    "        fold_results['test_mse'].append(test_error)\n",
    "        fold_results['naive_mse'].append(naive_error)\n",
    "\n",
    "        # Calculate average metrics across all windows\n",
    "        print(f'Fold Train MSE: {train_error}')\n",
    "        print(f'Fold Test MSE: {test_error}')\n",
    "        print(f'Naive Test MSE: {naive_error}')\n",
    "\n",
    "avg_train_mse = np.mean(fold_results['train_mse'])\n",
    "avg_test_mse = np.mean(fold_results['test_mse'])\n",
    "avg_naive_mse = np.mean(fold_results['naive_mse'])\n",
    "\n",
    "print()\n",
    "print(\"Sliding Window Cross-Validation Results\")\n",
    "print(f\"Average Train MSE: {avg_train_mse}\")\n",
    "print(f\"Average Test MSE: {avg_test_mse}\")\n",
    "print(f\"Average Naive MSE: {avg_naive_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LIST = [1]\n",
    "TEST_SIZE = 20\n",
    "TARGETS = df_ma.columns[:12]\n",
    "\n",
    "# Store results for each fold\n",
    "fold_results = {'train_mse': [], 'test_mse': [], 'naive_mse': []}\n",
    "\n",
    "for LAG in range(1, 2): # steps into the future\n",
    "    df_ma = ma_data_retrieval(window_list=WINDOW_LIST, lag=LAG)\n",
    "\n",
    "    for cnt in range(0, 20, 20): # sliding window\n",
    "        print()\n",
    "        print(f'WINDOW SLIDING: {cnt}, LAG: {LAG}')\n",
    "\n",
    "        df_window = df_ma[(len(df_ma)-cnt-250):(len(df_ma)-cnt)]\n",
    "\n",
    "X, y = df_window.drop(columns=TARGETS), df_window[TARGETS]\n",
    "\n",
    "X_train, X_test = X[:-TEST_SIZE], X[-TEST_SIZE:]\n",
    "y_train, y_test = y[:-TEST_SIZE], y[-TEST_SIZE:]\n",
    "\n",
    "df_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.21237847, 5.1619604 , 5.04709044, 4.67967744, 4.15193884,\n",
       "       3.71704813, 3.56014822, 3.47621561, 3.59151907, 3.69388815,\n",
       "       4.05337747, 3.97061296, 5.17205815, 5.12886437, 5.01424317,\n",
       "       4.66327922, 4.14612473, 3.72125655, 3.5663183 , 3.46866433,\n",
       "       3.58347315, 3.67848346, 4.03141349, 3.94227142, 5.13233578,\n",
       "       5.09728157, 4.98248951, 4.64939049, 4.14524081, 3.72981473,\n",
       "       3.5777566 , 3.4666548 , 3.57973041, 3.6675905 , 4.01431426,\n",
       "       3.91733525, 5.07160511, 5.04674751, 4.93306038, 4.62497583,\n",
       "       4.14623009, 3.74496875, 3.59395559, 3.47161179, 3.57972479,\n",
       "       3.66129636, 3.99697002, 3.893777  , 4.99595397, 4.9818709 ,\n",
       "       4.87144346, 4.59243931, 4.15340526, 3.7638003 , 3.61302775,\n",
       "       3.48571821, 3.58438334, 3.65794603, 3.9813363 , 3.87494375,\n",
       "       4.92350077, 4.91865864, 4.81355449, 4.56344598, 4.16675478,\n",
       "       3.79046157, 3.63740181, 3.50891515, 3.59565802, 3.65637864,\n",
       "       3.97176596, 3.86572954, 4.85906877, 4.85814391, 4.7626323 ,\n",
       "       4.53910555, 4.18442358, 3.82409219, 3.66441954, 3.53502384,\n",
       "       3.61093553, 3.65464092, 3.96200609, 3.85732179, 4.81259426,\n",
       "       4.81221   , 4.72378319, 4.5177934 , 4.19115324, 3.84139886,\n",
       "       3.68021944, 3.54974379, 3.61982058, 3.65015609, 3.95300209,\n",
       "       3.84833571, 4.77189425, 4.77137905, 4.68916032, 4.49787649,\n",
       "       4.19240162, 3.85277199, 3.69104826, 3.55883379, 3.62405416,\n",
       "       3.64255227, 3.94299828, 3.83915036, 4.74489187, 4.74491277,\n",
       "       4.66348315, 4.48511572, 4.18812405, 3.85619566, 3.69488161,\n",
       "       3.56345007, 3.62501435, 3.63616649, 3.93547358, 3.83229027,\n",
       "       4.72414217, 4.71967567, 4.64160466, 4.47282978, 4.18241825,\n",
       "       3.85638122, 3.69310249, 3.56469801, 3.62283395, 3.63096326,\n",
       "       3.92574571, 3.82730681, 4.70917627, 4.70254657, 4.62546696,\n",
       "       4.46223947, 4.17747648, 3.85456587, 3.69098025, 3.56433384,\n",
       "       3.6206419 , 3.62649884, 3.9210443 , 3.82425993, 4.69816212,\n",
       "       4.69065391, 4.61390288, 4.45485568, 4.17322546, 3.85166144,\n",
       "       3.688463  , 3.56428354, 3.61729289, 3.62353118, 3.91882991,\n",
       "       3.82366652, 4.68506871, 4.67525321, 4.60035441, 4.44535452,\n",
       "       4.16915433, 3.84820087, 3.68569031, 3.56312662, 3.61213391,\n",
       "       3.61909249, 3.91456945, 3.82154553, 4.6812842 , 4.67134004,\n",
       "       4.59557045, 4.44230171, 4.16759107, 3.84482826, 3.68232753,\n",
       "       3.56201843, 3.60767564, 3.61672437, 3.91659077, 3.82471672,\n",
       "       4.67530348, 4.66506272, 4.58949712, 4.43925876, 4.16555187,\n",
       "       3.84171291, 3.67981939, 3.56189194, 3.60245349, 3.61469656,\n",
       "       3.9162824 , 3.82617435, 4.67329246, 4.66061251, 4.58622155,\n",
       "       4.43797329, 4.16547563, 3.83992714, 3.67721911, 3.56287905,\n",
       "       3.5987054 , 3.61385151, 3.91726049, 3.82955681, 4.67851142,\n",
       "       4.66402278, 4.58934995, 4.43971403, 4.16798722, 3.83912523,\n",
       "       3.67446899, 3.56495657, 3.59901039, 3.61493184, 3.92313412,\n",
       "       3.83711951, 4.70608564, 4.70012104, 4.61466111, 4.45947144,\n",
       "       4.17434201, 3.83748734, 3.67175455, 3.5708413 , 3.60734346,\n",
       "       3.62514963, 3.94691179, 3.85908474, 4.74156645, 4.73915501,\n",
       "       4.64526033, 4.48273934, 4.18461628, 3.83824973, 3.67102   ,\n",
       "       3.58028642, 3.62017892, 3.64004843, 3.97309449, 3.88566729])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(dataset['test_input']).cpu().detach().numpy().flatten()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 Mo</th>\n",
       "      <th>2 Mo</th>\n",
       "      <th>3 Mo</th>\n",
       "      <th>6 Mo</th>\n",
       "      <th>1 Yr</th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>3 Yr</th>\n",
       "      <th>5 Yr</th>\n",
       "      <th>7 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "      <th>20 Yr</th>\n",
       "      <th>30 Yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-10</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1 Mo  2 Mo  3 Mo  6 Mo  1 Yr  2 Yr  3 Yr  5 Yr  7 Yr  10 Yr  \\\n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "2024-09-10  5.18  5.18  5.06  4.65  4.07  3.59  3.42  3.43  3.53   3.65   \n",
       "\n",
       "            20 Yr  30 Yr  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  \n",
       "2024-09-10   4.04   3.97  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 Mo</th>\n",
       "      <th>2 Mo</th>\n",
       "      <th>3 Mo</th>\n",
       "      <th>6 Mo</th>\n",
       "      <th>1 Yr</th>\n",
       "      <th>2 Yr</th>\n",
       "      <th>3 Yr</th>\n",
       "      <th>5 Yr</th>\n",
       "      <th>7 Yr</th>\n",
       "      <th>10 Yr</th>\n",
       "      <th>20 Yr</th>\n",
       "      <th>30 Yr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-11</th>\n",
       "      <td>5.21</td>\n",
       "      <td>5.19</td>\n",
       "      <td>5.10</td>\n",
       "      <td>4.72</td>\n",
       "      <td>4.12</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.03</td>\n",
       "      <td>3.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-12</th>\n",
       "      <td>5.18</td>\n",
       "      <td>5.22</td>\n",
       "      <td>5.06</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.09</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.68</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-13</th>\n",
       "      <td>5.15</td>\n",
       "      <td>5.17</td>\n",
       "      <td>4.97</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.66</td>\n",
       "      <td>4.05</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-16</th>\n",
       "      <td>5.11</td>\n",
       "      <td>5.10</td>\n",
       "      <td>4.96</td>\n",
       "      <td>4.55</td>\n",
       "      <td>3.96</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4.01</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-17</th>\n",
       "      <td>5.05</td>\n",
       "      <td>5.05</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.55</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.65</td>\n",
       "      <td>4.02</td>\n",
       "      <td>3.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18</th>\n",
       "      <td>4.91</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.70</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-19</th>\n",
       "      <td>4.89</td>\n",
       "      <td>4.91</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.46</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.11</td>\n",
       "      <td>4.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-20</th>\n",
       "      <td>4.87</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.43</td>\n",
       "      <td>3.92</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.46</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.59</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-23</th>\n",
       "      <td>4.85</td>\n",
       "      <td>4.84</td>\n",
       "      <td>4.72</td>\n",
       "      <td>4.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.57</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-24</th>\n",
       "      <td>4.78</td>\n",
       "      <td>4.78</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-25</th>\n",
       "      <td>4.79</td>\n",
       "      <td>4.78</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.79</td>\n",
       "      <td>4.18</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-26</th>\n",
       "      <td>4.90</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.96</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.79</td>\n",
       "      <td>4.17</td>\n",
       "      <td>4.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-27</th>\n",
       "      <td>4.90</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.35</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.49</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.15</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-30</th>\n",
       "      <td>4.93</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.73</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.98</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.81</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-01</th>\n",
       "      <td>4.96</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.96</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.60</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-02</th>\n",
       "      <td>4.92</td>\n",
       "      <td>4.83</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.36</td>\n",
       "      <td>3.97</td>\n",
       "      <td>3.63</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.79</td>\n",
       "      <td>4.19</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-03</th>\n",
       "      <td>4.99</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.68</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.02</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.62</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-04</th>\n",
       "      <td>5.01</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.73</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.20</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.84</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.98</td>\n",
       "      <td>4.33</td>\n",
       "      <td>4.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-07</th>\n",
       "      <td>5.00</td>\n",
       "      <td>4.87</td>\n",
       "      <td>4.77</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.24</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.92</td>\n",
       "      <td>4.03</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-08</th>\n",
       "      <td>4.96</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.44</td>\n",
       "      <td>4.21</td>\n",
       "      <td>3.98</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.86</td>\n",
       "      <td>3.94</td>\n",
       "      <td>4.04</td>\n",
       "      <td>4.38</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1 Mo  2 Mo  3 Mo  6 Mo  1 Yr  2 Yr  3 Yr  5 Yr  7 Yr  10 Yr  \\\n",
       "Date                                                                      \n",
       "2024-09-11  5.21  5.19  5.10  4.72  4.12  3.62  3.45  3.45  3.54   3.65   \n",
       "2024-09-12  5.18  5.22  5.06  4.68  4.09  3.64  3.47  3.47  3.57   3.68   \n",
       "2024-09-13  5.15  5.17  4.97  4.60  4.00  3.57  3.42  3.43  3.53   3.66   \n",
       "2024-09-16  5.11  5.10  4.96  4.55  3.96  3.56  3.42  3.41  3.51   3.63   \n",
       "2024-09-17  5.05  5.05  4.95  4.55  3.99  3.59  3.45  3.44  3.53   3.65   \n",
       "2024-09-18  4.91  4.91  4.84  4.50  3.95  3.61  3.49  3.47  3.58   3.70   \n",
       "2024-09-19  4.89  4.91  4.80  4.46  3.93  3.59  3.47  3.49  3.60   3.73   \n",
       "2024-09-20  4.87  4.88  4.75  4.43  3.92  3.55  3.46  3.48  3.59   3.73   \n",
       "2024-09-23  4.85  4.84  4.72  4.40  3.91  3.57  3.47  3.51  3.62   3.75   \n",
       "2024-09-24  4.78  4.78  4.69  4.36  3.88  3.49  3.44  3.47  3.60   3.74   \n",
       "2024-09-25  4.79  4.78  4.69  4.36  3.89  3.53  3.49  3.52  3.65   3.79   \n",
       "2024-09-26  4.90  4.87  4.68  4.38  3.96  3.60  3.54  3.55  3.65   3.79   \n",
       "2024-09-27  4.90  4.87  4.68  4.35  3.90  3.55  3.49  3.50  3.60   3.75   \n",
       "2024-09-30  4.93  4.87  4.73  4.38  3.98  3.66  3.58  3.58  3.67   3.81   \n",
       "2024-10-01  4.96  4.87  4.71  4.36  3.96  3.61  3.52  3.51  3.60   3.74   \n",
       "2024-10-02  4.92  4.83  4.69  4.36  3.97  3.63  3.54  3.55  3.65   3.79   \n",
       "2024-10-03  4.99  4.85  4.68  4.37  4.02  3.70  3.62  3.62  3.71   3.85   \n",
       "2024-10-04  5.01  4.88  4.73  4.45  4.20  3.93  3.84  3.81  3.88   3.98   \n",
       "2024-10-07  5.00  4.87  4.77  4.45  4.24  3.99  3.89  3.86  3.92   4.03   \n",
       "2024-10-08  4.96  4.85  4.75  4.44  4.21  3.98  3.86  3.86  3.94   4.04   \n",
       "\n",
       "            20 Yr  30 Yr  \n",
       "Date                      \n",
       "2024-09-11   4.03   3.96  \n",
       "2024-09-12   4.07   4.00  \n",
       "2024-09-13   4.05   3.98  \n",
       "2024-09-16   4.01   3.94  \n",
       "2024-09-17   4.02   3.96  \n",
       "2024-09-18   4.08   4.03  \n",
       "2024-09-19   4.11   4.06  \n",
       "2024-09-20   4.10   4.07  \n",
       "2024-09-23   4.12   4.09  \n",
       "2024-09-24   4.13   4.09  \n",
       "2024-09-25   4.18   4.14  \n",
       "2024-09-26   4.17   4.12  \n",
       "2024-09-27   4.15   4.10  \n",
       "2024-09-30   4.19   4.14  \n",
       "2024-10-01   4.14   4.08  \n",
       "2024-10-02   4.19   4.14  \n",
       "2024-10-03   4.24   4.18  \n",
       "2024-10-04   4.33   4.26  \n",
       "2024-10-07   4.37   4.30  \n",
       "2024-10-08   4.38   4.32  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.28945350806879727)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(df_naive.values.flatten(), y_test.values.flatten(), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.96 4.85 4.75 4.44 4.21 3.98 3.86 3.86 3.94 4.04 4.38 4.32]\n",
      "[4.68 4.71 4.64 4.41 4.31 4.27 4.2  4.27 4.37 4.42 4.71 4.6 ]\n"
     ]
    }
   ],
   "source": [
    "x1 = df_naive.values.flatten()\n",
    "x2 = y_test.values.flatten()\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float('x', -10, 10)\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "study.best_params  # E.g. {'x': 2.002108042}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "\n",
    "def train_mse(model, dataset):\n",
    "    predictions = model(dataset['train_input'])  # Model predictions\n",
    "    loss = torch.nn.functional.mse_loss(predictions, dataset['train_label'])\n",
    "    return loss\n",
    "\n",
    "def test_mse(model, dataset):\n",
    "    predictions = model(dataset['test_input'])  # Model predictions\n",
    "    loss = torch.nn.functional.mse_loss(predictions, dataset['test_label'])\n",
    "    return loss\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 2)  # Number of layers in the network\n",
    "    layer_sizes = [trial.suggest_int(f'n_units_l{i}', 16, 64, step=16) for i in range(n_layers)]\n",
    "    grid = trial.suggest_int('grid', 2, 4)          # Example parameter for KAN\n",
    "    lamb = trial.suggest_float('lamb', 1e-4, 1e-2, log=True)  # Regularization rate\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)       # Learning rate\n",
    "    steps = trial.suggest_int('steps', 500, 2000, step=500)   # Training steps\n",
    "\n",
    "    # Model architecture\n",
    "    width = [n_inputs] + layer_sizes + [n_outputs]\n",
    "\n",
    "    # Initialize dataset\n",
    "    dataset = dict()\n",
    "    dtype = torch.get_default_dtype()\n",
    "    dataset['train_input'] = torch.from_numpy(X_train.values).type(dtype).to(device)\n",
    "    dataset['train_label'] = torch.from_numpy(y_train.values).type(dtype).to(device)\n",
    "    dataset['test_input'] = torch.from_numpy(X_test.values).type(dtype).to(device)\n",
    "    dataset['test_label'] = torch.from_numpy(y_test.values).type(dtype).to(device)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = KAN(width=width, grid=grid, k=2, seed=42, device=device)\n",
    "\n",
    "    # Train the model\n",
    "    results = model.fit(\n",
    "        dataset, \n",
    "        opt=\"Adam\", \n",
    "        lamb=lamb, \n",
    "        lr=lr, \n",
    "        steps=steps, \n",
    "        metrics=(lambda: train_mse(model, dataset), lambda: test_mse(model, dataset))\n",
    "    )\n",
    "\n",
    "    # Retrieve the metric (e.g., test MSE) from the results\n",
    "    test_mse_value = results['test_loss'][-1]\n",
    "    return test_mse_value  # Minimize test MSE\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best parameters and results\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best test MSE:\", study.best_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
