{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spline-related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B_batch(x, grid, k=0, extend=True, device='cpu'):\n",
    "    '''\n",
    "    evaludate x on B-spline bases\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        x : 2D torch.tensor\n",
    "            inputs, shape (number of splines, number of samples)\n",
    "        grid : 2D torch.tensor\n",
    "            grids, shape (number of splines, number of grid points)\n",
    "        k : int\n",
    "            the piecewise polynomial order of splines.\n",
    "        extend : bool\n",
    "            If True, k points are extended on both ends. If False, no extension (zero boundary condition). Default: True\n",
    "        device : str\n",
    "            devicde\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        spline values : 3D torch.tensor\n",
    "            shape (batch, in_dim, G+k). G: the number of grid intervals, k: spline order.\n",
    "      \n",
    "    Example\n",
    "    -------\n",
    "    >>> from kan.spline import B_batch\n",
    "    >>> x = torch.rand(100,2)\n",
    "    >>> grid = torch.linspace(-1,1,steps=11)[None, :].expand(2, 11)\n",
    "    >>> B_batch(x, grid, k=3).shape\n",
    "    '''\n",
    "    \n",
    "    x = x.unsqueeze(dim=2)\n",
    "    grid = grid.unsqueeze(dim=0)\n",
    "    \n",
    "    if k == 0:\n",
    "        value = (x >= grid[:, :, :-1]) * (x < grid[:, :, 1:])\n",
    "    else:\n",
    "        B_km1 = B_batch(x[:,:,0], grid=grid[0], k=k - 1)\n",
    "        \n",
    "        value = (x - grid[:, :, :-(k + 1)]) / (grid[:, :, k:-1] - grid[:, :, :-(k + 1)]) * B_km1[:, :, :-1] + (\n",
    "                    grid[:, :, k + 1:] - x) / (grid[:, :, k + 1:] - grid[:, :, 1:(-k)]) * B_km1[:, :, 1:]\n",
    "    \n",
    "    # in case grid is degenerate\n",
    "    value = torch.nan_to_num(value)\n",
    "    return value\n",
    "\n",
    "def extend_grid(grid, k_extend=0):\n",
    "    '''\n",
    "    extend grid\n",
    "    '''\n",
    "    h = (grid[:, [-1]] - grid[:, [0]]) / (grid.shape[1] - 1)\n",
    "\n",
    "    for i in range(k_extend):\n",
    "        grid = torch.cat([grid[:, [0]] - h, grid], dim=1)\n",
    "        grid = torch.cat([grid, grid[:, [-1]] + h], dim=1)\n",
    "\n",
    "    return grid\n",
    "\n",
    "def coef2curve(x_eval, grid, coef, k, device=\"cpu\"):\n",
    "    '''\n",
    "    converting B-spline coefficients to B-spline curves. Evaluate x on B-spline curves (summing up B_batch results over B-spline basis).\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        x_eval : 2D torch.tensor\n",
    "            shape (batch, in_dim)\n",
    "        grid : 2D torch.tensor\n",
    "            shape (in_dim, G+2k). G: the number of grid intervals; k: spline order.\n",
    "        coef : 3D torch.tensor\n",
    "            shape (in_dim, out_dim, G+k)\n",
    "        k : int\n",
    "            the piecewise polynomial order of splines.\n",
    "        device : str\n",
    "            devicde\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "        y_eval : 3D torch.tensor\n",
    "            shape (batch, in_dim, out_dim)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    b_splines = B_batch(x_eval, grid, k=k)\n",
    "    y_eval = torch.einsum('ijk,jlk->ijl', b_splines, coef.to(b_splines.device))\n",
    "    \n",
    "    return y_eval\n",
    "\n",
    "def curve2coef(x_eval, y_eval, grid, k):\n",
    "    '''\n",
    "    converting B-spline curves to B-spline coefficients using least squares.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        x_eval : 2D torch.tensor\n",
    "            shape (batch, in_dim)\n",
    "        y_eval : 3D torch.tensor\n",
    "            shape (batch, in_dim, out_dim)\n",
    "        grid : 2D torch.tensor\n",
    "            shape (in_dim, grid+2*k)\n",
    "        k : int\n",
    "            spline order\n",
    "        lamb : float\n",
    "            regularized least square lambda\n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "        coef : 3D torch.tensor\n",
    "            shape (in_dim, out_dim, G+k)\n",
    "    '''\n",
    "    #print('haha', x_eval.shape, y_eval.shape, grid.shape)\n",
    "    batch = x_eval.shape[0]\n",
    "    in_dim = x_eval.shape[1]\n",
    "    out_dim = y_eval.shape[2]\n",
    "    n_coef = grid.shape[1] - k - 1\n",
    "    mat = B_batch(x_eval, grid, k)\n",
    "    mat = mat.permute(1,0,2)[:,None,:,:].expand(in_dim, out_dim, batch, n_coef)\n",
    "    #print('mat', mat.shape)\n",
    "    y_eval = y_eval.permute(1,2,0).unsqueeze(dim=3)\n",
    "    #print('y_eval', y_eval.shape)\n",
    "    device = mat.device\n",
    "    \n",
    "    #coef = torch.linalg.lstsq(mat, y_eval, driver='gelsy' if device == 'cpu' else 'gels').solution[:,:,:,0]\n",
    "    try:\n",
    "        coef = torch.linalg.lstsq(mat, y_eval).solution[:,:,:,0]\n",
    "    except:\n",
    "        print('lstsq failed')\n",
    "    \n",
    "    # manual psuedo-inverse\n",
    "    '''lamb=1e-8\n",
    "    XtX = torch.einsum('ijmn,ijnp->ijmp', mat.permute(0,1,3,2), mat)\n",
    "    Xty = torch.einsum('ijmn,ijnp->ijmp', mat.permute(0,1,3,2), y_eval)\n",
    "    n1, n2, n = XtX.shape[0], XtX.shape[1], XtX.shape[2]\n",
    "    identity = torch.eye(n,n)[None, None, :, :].expand(n1, n2, n, n).to(device)\n",
    "    A = XtX + lamb * identity\n",
    "    B = Xty\n",
    "    coef = (A.pinverse() @ B)[:,:,:,0]'''\n",
    "    \n",
    "    return coef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KAN Layer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from .spline import *\n",
    "from .utils import sparse_mask\n",
    "\n",
    "\n",
    "class KANLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    KANLayer class\n",
    "    \n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "        in_dim: int\n",
    "            input dimension\n",
    "        out_dim: int\n",
    "            output dimension\n",
    "        num: int\n",
    "            the number of grid intervals\n",
    "        k: int\n",
    "            the piecewise polynomial order of splines\n",
    "        noise_scale: float\n",
    "            spline scale at initialization\n",
    "        coef: 2D torch.tensor\n",
    "            coefficients of B-spline bases\n",
    "        scale_base_mu: float\n",
    "            magnitude of the residual function b(x) is drawn from N(mu, sigma^2), mu = sigma_base_mu\n",
    "        scale_base_sigma: float\n",
    "            magnitude of the residual function b(x) is drawn from N(mu, sigma^2), mu = sigma_base_sigma\n",
    "        scale_sp: float\n",
    "            mangitude of the spline function spline(x)\n",
    "        base_fun: fun\n",
    "            residual function b(x)\n",
    "        mask: 1D torch.float\n",
    "            mask of spline functions. setting some element of the mask to zero means setting the corresponding activation to zero function.\n",
    "        grid_eps: float in [0,1]\n",
    "            a hyperparameter used in update_grid_from_samples. When grid_eps = 1, the grid is uniform; when grid_eps = 0, the grid is partitioned using percentiles of samples. 0 < grid_eps < 1 interpolates between the two extremes.\n",
    "            the id of activation functions that are locked\n",
    "        device: str\n",
    "            device\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim=3, out_dim=2, num=5, k=3, noise_scale=0.5, scale_base_mu=0.0, scale_base_sigma=1.0, scale_sp=1.0, base_fun=torch.nn.SiLU(), grid_eps=0.02, grid_range=[-1, 1], sp_trainable=True, sb_trainable=True, save_plot_data = True, device='cpu', sparse_init=False):\n",
    "        ''''\n",
    "        initialize a KANLayer\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            in_dim : int\n",
    "                input dimension. Default: 2.\n",
    "            out_dim : int\n",
    "                output dimension. Default: 3.\n",
    "            num : int\n",
    "                the number of grid intervals = G. Default: 5.\n",
    "            k : int\n",
    "                the order of piecewise polynomial. Default: 3.\n",
    "            noise_scale : float\n",
    "                the scale of noise injected at initialization. Default: 0.1.\n",
    "            scale_base_mu : float\n",
    "                the scale of the residual function b(x) is intialized to be N(scale_base_mu, scale_base_sigma^2).\n",
    "            scale_base_sigma : float\n",
    "                the scale of the residual function b(x) is intialized to be N(scale_base_mu, scale_base_sigma^2).\n",
    "            scale_sp : float\n",
    "                the scale of the base function spline(x).\n",
    "            base_fun : function\n",
    "                residual function b(x). Default: torch.nn.SiLU()\n",
    "            grid_eps : float\n",
    "                When grid_eps = 1, the grid is uniform; when grid_eps = 0, the grid is partitioned using percentiles of samples. 0 < grid_eps < 1 interpolates between the two extremes.\n",
    "            grid_range : list/np.array of shape (2,)\n",
    "                setting the range of grids. Default: [-1,1].\n",
    "            sp_trainable : bool\n",
    "                If true, scale_sp is trainable\n",
    "            sb_trainable : bool\n",
    "                If true, scale_base is trainable\n",
    "            device : str\n",
    "                device\n",
    "            sparse_init : bool\n",
    "                if sparse_init = True, sparse initialization is applied.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            self\n",
    "            \n",
    "        Example\n",
    "        -------\n",
    "        >>> from kan.KANLayer import *\n",
    "        >>> model = KANLayer(in_dim=3, out_dim=5)\n",
    "        >>> (model.in_dim, model.out_dim)\n",
    "        '''\n",
    "        super(KANLayer, self).__init__()\n",
    "        # size \n",
    "        self.out_dim = out_dim\n",
    "        self.in_dim = in_dim\n",
    "        self.num = num\n",
    "        self.k = k\n",
    "\n",
    "        grid = torch.linspace(grid_range[0], grid_range[1], steps=num + 1)[None,:].expand(self.in_dim, num+1)\n",
    "        grid = extend_grid(grid, k_extend=k)\n",
    "        self.grid = torch.nn.Parameter(grid).requires_grad_(False)\n",
    "        noises = (torch.rand(self.num+1, self.in_dim, self.out_dim) - 1/2) * noise_scale / num\n",
    "\n",
    "        self.coef = torch.nn.Parameter(curve2coef(self.grid[:,k:-k].permute(1,0), noises, self.grid, k))\n",
    "        \n",
    "        if sparse_init:\n",
    "            self.mask = torch.nn.Parameter(sparse_mask(in_dim, out_dim)).requires_grad_(False)\n",
    "        else:\n",
    "            self.mask = torch.nn.Parameter(torch.ones(in_dim, out_dim)).requires_grad_(False)\n",
    "        \n",
    "        self.scale_base = torch.nn.Parameter(scale_base_mu * 1 / np.sqrt(in_dim) + \\\n",
    "                         scale_base_sigma * (torch.rand(in_dim, out_dim)*2-1) * 1/np.sqrt(in_dim)).requires_grad_(sb_trainable)\n",
    "        self.scale_sp = torch.nn.Parameter(torch.ones(in_dim, out_dim) * scale_sp * 1 / np.sqrt(in_dim) * self.mask).requires_grad_(sp_trainable)  # make scale trainable\n",
    "        self.base_fun = base_fun\n",
    "\n",
    "        \n",
    "        self.grid_eps = grid_eps\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def to(self, device):\n",
    "        super(KANLayer, self).to(device)\n",
    "        self.device = device    \n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        KANLayer forward given input x\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            x : 2D torch.float\n",
    "                inputs, shape (number of samples, input dimension)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            y : 2D torch.float\n",
    "                outputs, shape (number of samples, output dimension)\n",
    "            preacts : 3D torch.float\n",
    "                fan out x into activations, shape (number of sampels, output dimension, input dimension)\n",
    "            postacts : 3D torch.float\n",
    "                the outputs of activation functions with preacts as inputs\n",
    "            postspline : 3D torch.float\n",
    "                the outputs of spline functions with preacts as inputs\n",
    "        \n",
    "        Example\n",
    "        -------\n",
    "        >>> from kan.KANLayer import *\n",
    "        >>> model = KANLayer(in_dim=3, out_dim=5)\n",
    "        >>> x = torch.normal(0,1,size=(100,3))\n",
    "        >>> y, preacts, postacts, postspline = model(x)\n",
    "        >>> y.shape, preacts.shape, postacts.shape, postspline.shape\n",
    "        '''\n",
    "        batch = x.shape[0]\n",
    "        preacts = x[:,None,:].clone().expand(batch, self.out_dim, self.in_dim)\n",
    "            \n",
    "        base = self.base_fun(x) # (batch, in_dim)\n",
    "        y = coef2curve(x_eval=x, grid=self.grid, coef=self.coef, k=self.k)\n",
    "        \n",
    "        postspline = y.clone().permute(0,2,1)\n",
    "            \n",
    "        y = self.scale_base[None,:,:] * base[:,:,None] + self.scale_sp[None,:,:] * y\n",
    "        y = self.mask[None,:,:] * y\n",
    "        \n",
    "        postacts = y.clone().permute(0,2,1)\n",
    "            \n",
    "        y = torch.sum(y, dim=1)\n",
    "        return y, preacts, postacts, postspline\n",
    "\n",
    "    def update_grid_from_samples(self, x, mode='sample'):\n",
    "        '''\n",
    "        update grid from samples\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            x : 2D torch.float\n",
    "                inputs, shape (number of samples, input dimension)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            None\n",
    "        \n",
    "        Example\n",
    "        -------\n",
    "        >>> model = KANLayer(in_dim=1, out_dim=1, num=5, k=3)\n",
    "        >>> print(model.grid.data)\n",
    "        >>> x = torch.linspace(-3,3,steps=100)[:,None]\n",
    "        >>> model.update_grid_from_samples(x)\n",
    "        >>> print(model.grid.data)\n",
    "        '''\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        #x = torch.einsum('ij,k->ikj', x, torch.ones(self.out_dim, ).to(self.device)).reshape(batch, self.size).permute(1, 0)\n",
    "        x_pos = torch.sort(x, dim=0)[0]\n",
    "        y_eval = coef2curve(x_pos, self.grid, self.coef, self.k)\n",
    "        num_interval = self.grid.shape[1] - 1 - 2*self.k\n",
    "        \n",
    "        def get_grid(num_interval):\n",
    "            ids = [int(batch / num_interval * i) for i in range(num_interval)] + [-1]\n",
    "            grid_adaptive = x_pos[ids, :].permute(1,0)\n",
    "            margin = 0.00\n",
    "            h = (grid_adaptive[:,[-1]] - grid_adaptive[:,[0]] + 2 * margin)/num_interval\n",
    "            grid_uniform = grid_adaptive[:,[0]] - margin + h * torch.arange(num_interval+1,)[None, :].to(x.device)\n",
    "            grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "            return grid\n",
    "        \n",
    "        \n",
    "        grid = get_grid(num_interval)\n",
    "        \n",
    "        if mode == 'grid':\n",
    "            sample_grid = get_grid(2*num_interval)\n",
    "            x_pos = sample_grid.permute(1,0)\n",
    "            y_eval = coef2curve(x_pos, self.grid, self.coef, self.k)\n",
    "        \n",
    "        self.grid.data = extend_grid(grid, k_extend=self.k)\n",
    "        #print('x_pos 2', x_pos.shape)\n",
    "        #print('y_eval 2', y_eval.shape)\n",
    "        self.coef.data = curve2coef(x_pos, y_eval, self.grid, self.k)\n",
    "\n",
    "    def initialize_grid_from_parent(self, parent, x, mode='sample'):\n",
    "        '''\n",
    "        update grid from a parent KANLayer & samples\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            parent : KANLayer\n",
    "                a parent KANLayer (whose grid is usually coarser than the current model)\n",
    "            x : 2D torch.float\n",
    "                inputs, shape (number of samples, input dimension)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            None\n",
    "          \n",
    "        Example\n",
    "        -------\n",
    "        >>> batch = 100\n",
    "        >>> parent_model = KANLayer(in_dim=1, out_dim=1, num=5, k=3)\n",
    "        >>> print(parent_model.grid.data)\n",
    "        >>> model = KANLayer(in_dim=1, out_dim=1, num=10, k=3)\n",
    "        >>> x = torch.normal(0,1,size=(batch, 1))\n",
    "        >>> model.initialize_grid_from_parent(parent_model, x)\n",
    "        >>> print(model.grid.data)\n",
    "        '''\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        \n",
    "        # shrink grid\n",
    "        x_pos = torch.sort(x, dim=0)[0]\n",
    "        y_eval = coef2curve(x_pos, parent.grid, parent.coef, parent.k)\n",
    "        num_interval = self.grid.shape[1] - 1 - 2*self.k\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        # based on samples\n",
    "        def get_grid(num_interval):\n",
    "            ids = [int(batch / num_interval * i) for i in range(num_interval)] + [-1]\n",
    "            grid_adaptive = x_pos[ids, :].permute(1,0)\n",
    "            h = (grid_adaptive[:,[-1]] - grid_adaptive[:,[0]])/num_interval\n",
    "            grid_uniform = grid_adaptive[:,[0]] + h * torch.arange(num_interval+1,)[None, :].to(x.device)\n",
    "            grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "            return grid'''\n",
    "        \n",
    "        #print('p', parent.grid)\n",
    "        # based on interpolating parent grid\n",
    "        def get_grid(num_interval):\n",
    "            x_pos = parent.grid[:,parent.k:-parent.k]\n",
    "            #print('x_pos', x_pos)\n",
    "            sp2 = KANLayer(in_dim=1, out_dim=self.in_dim,k=1,num=x_pos.shape[1]-1,scale_base_mu=0.0, scale_base_sigma=0.0).to(x.device)\n",
    "\n",
    "            #print('sp2_grid', sp2.grid[:,sp2.k:-sp2.k].permute(1,0).expand(-1,self.in_dim))\n",
    "            #print('sp2_coef_shape', sp2.coef.shape)\n",
    "            sp2_coef = curve2coef(sp2.grid[:,sp2.k:-sp2.k].permute(1,0).expand(-1,self.in_dim), x_pos.permute(1,0).unsqueeze(dim=2), sp2.grid[:,:], k=1).permute(1,0,2)\n",
    "            shp = sp2_coef.shape\n",
    "            #sp2_coef = torch.cat([torch.zeros(shp[0], shp[1], 1), sp2_coef, torch.zeros(shp[0], shp[1], 1)], dim=2)\n",
    "            #print('sp2_coef',sp2_coef)\n",
    "            #print(sp2.coef.shape)\n",
    "            sp2.coef.data = sp2_coef\n",
    "            percentile = torch.linspace(-1,1,self.num+1).to(self.device)\n",
    "            grid = sp2(percentile.unsqueeze(dim=1))[0].permute(1,0)\n",
    "            #print('c', grid)\n",
    "            return grid\n",
    "        \n",
    "        grid = get_grid(num_interval)\n",
    "        \n",
    "        if mode == 'grid':\n",
    "            sample_grid = get_grid(2*num_interval)\n",
    "            x_pos = sample_grid.permute(1,0)\n",
    "            y_eval = coef2curve(x_pos, parent.grid, parent.coef, parent.k)\n",
    "        \n",
    "        grid = extend_grid(grid, k_extend=self.k)\n",
    "        self.grid.data = grid\n",
    "        self.coef.data = curve2coef(x_pos, y_eval, self.grid, self.k)\n",
    "\n",
    "    def get_subset(self, in_id, out_id):\n",
    "        '''\n",
    "        get a smaller KANLayer from a larger KANLayer (used for pruning)\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            in_id : list\n",
    "                id of selected input neurons\n",
    "            out_id : list\n",
    "                id of selected output neurons\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            spb : KANLayer\n",
    "            \n",
    "        Example\n",
    "        -------\n",
    "        >>> kanlayer_large = KANLayer(in_dim=10, out_dim=10, num=5, k=3)\n",
    "        >>> kanlayer_small = kanlayer_large.get_subset([0,9],[1,2,3])\n",
    "        >>> kanlayer_small.in_dim, kanlayer_small.out_dim\n",
    "        (2, 3)\n",
    "        '''\n",
    "        spb = KANLayer(len(in_id), len(out_id), self.num, self.k, base_fun=self.base_fun)\n",
    "        spb.grid.data = self.grid[in_id]\n",
    "        spb.coef.data = self.coef[in_id][:,out_id]\n",
    "        spb.scale_base.data = self.scale_base[in_id][:,out_id]\n",
    "        spb.scale_sp.data = self.scale_sp[in_id][:,out_id]\n",
    "        spb.mask.data = self.mask[in_id][:,out_id]\n",
    "\n",
    "        spb.in_dim = len(in_id)\n",
    "        spb.out_dim = len(out_id)\n",
    "        return spb\n",
    "    \n",
    "    \n",
    "    def swap(self, i1, i2, mode='in'):\n",
    "        '''\n",
    "        swap the i1 neuron with the i2 neuron in input (if mode == 'in') or output (if mode == 'out') \n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            i1 : int\n",
    "            i2 : int\n",
    "            mode : str\n",
    "                mode = 'in' or 'out'\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            None\n",
    "            \n",
    "        Example\n",
    "        -------\n",
    "        >>> from kan.KANLayer import *\n",
    "        >>> model = KANLayer(in_dim=2, out_dim=2, num=5, k=3)\n",
    "        >>> print(model.coef)\n",
    "        >>> model.swap(0,1,mode='in')\n",
    "        >>> print(model.coef)\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            def swap_(data, i1, i2, mode='in'):\n",
    "                if mode == 'in':\n",
    "                    data[i1], data[i2] = data[i2].clone(), data[i1].clone()\n",
    "                elif mode == 'out':\n",
    "                    data[:,i1], data[:,i2] = data[:,i2].clone(), data[:,i1].clone()\n",
    "\n",
    "            if mode == 'in':\n",
    "                swap_(self.grid.data, i1, i2, mode='in')\n",
    "            swap_(self.coef.data, i1, i2, mode=mode)\n",
    "            swap_(self.scale_base.data, i1, i2, mode=mode)\n",
    "            swap_(self.scale_sp.data, i1, i2, mode=mode)\n",
    "            swap_(self.mask.data, i1, i2, mode=mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 5]),\n",
       " torch.Size([10, 5, 3]),\n",
       " torch.Size([10, 5, 3]),\n",
       " torch.Size([10, 5, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KANLayer(in_dim=3, out_dim=5)\n",
    "x = torch.normal(0,1,size=(10,3))\n",
    "y, preacts, postacts, postspline = model(x)\n",
    "y.shape, preacts.shape, postacts.shape, postspline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.2000, -1.8000, -1.4000, -1.0000, -0.6000, -0.2000,  0.2000,  0.6000,\n",
       "          1.0000,  1.4000,  1.8000,  2.2000],\n",
       "        [-2.2000, -1.8000, -1.4000, -1.0000, -0.6000, -0.2000,  0.2000,  0.6000,\n",
       "          1.0000,  1.4000,  1.8000,  2.2000],\n",
       "        [-2.2000, -1.8000, -1.4000, -1.0000, -0.6000, -0.2000,  0.2000,  0.6000,\n",
       "          1.0000,  1.4000,  1.8000,  2.2000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1577,  0.2098,  0.8069],\n",
       "        [-0.3472,  0.9898, -1.3866],\n",
       "        [ 2.5910, -0.9172,  0.9313],\n",
       "        [-1.9412, -0.1885,  0.6174],\n",
       "        [ 1.8303, -0.1185,  0.7934],\n",
       "        [ 0.5675, -1.3661,  1.5632],\n",
       "        [ 0.5183, -0.8264, -1.0139],\n",
       "        [-0.4844,  0.2779, -0.2367],\n",
       "        [ 0.5974, -1.4928, -1.8816],\n",
       "        [ 0.4388,  0.1251, -0.8757]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.7244e-02,  2.0984e-02,  3.1399e-02,  2.2120e-01, -7.4917e-02],\n",
       "        [ 2.2105e-01,  1.3360e-01,  1.2514e-01, -3.4961e-02, -2.3607e-01],\n",
       "        [-1.1613e+00, -6.1719e-01,  7.0836e-01,  1.3818e+00, -4.8807e-01],\n",
       "        [ 1.7140e-01,  1.1064e-01, -5.4891e-02,  1.1370e-03,  5.7152e-02],\n",
       "        [-7.1880e-01, -3.6137e-01,  5.0905e-01,  9.6794e-01, -3.5186e-01],\n",
       "        [-4.5690e-02, -3.3452e-02,  4.3541e-02,  5.8902e-01, -9.2564e-02],\n",
       "        [-2.3495e-01, -1.4181e-01,  8.4621e-02,  3.6548e-02,  9.7733e-03],\n",
       "        [ 8.7276e-02,  5.1072e-02,  1.9832e-02, -9.1225e-02, -1.1589e-02],\n",
       "        [-2.9184e-01, -1.3167e-01,  7.9335e-02,  7.5553e-02, -5.5596e-03],\n",
       "        [-1.6121e-01, -7.2817e-02,  1.0928e-01,  4.6068e-02, -5.2629e-02]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(f, \n",
    "                   n_var=2, \n",
    "                   f_mode = 'col',\n",
    "                   ranges = [-1,1],\n",
    "                   train_num=1000, \n",
    "                   test_num=1000,\n",
    "                   normalize_input=False,\n",
    "                   normalize_label=False,\n",
    "                   device='cpu',\n",
    "                   seed=0):\n",
    "    '''\n",
    "    create dataset\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        f : function\n",
    "            the symbolic formula used to create the synthetic dataset\n",
    "        ranges : list or np.array; shape (2,) or (n_var, 2)\n",
    "            the range of input variables. Default: [-1,1].\n",
    "        train_num : int\n",
    "            the number of training samples. Default: 1000.\n",
    "        test_num : int\n",
    "            the number of test samples. Default: 1000.\n",
    "        normalize_input : bool\n",
    "            If True, apply normalization to inputs. Default: False.\n",
    "        normalize_label : bool\n",
    "            If True, apply normalization to labels. Default: False.\n",
    "        device : str\n",
    "            device. Default: 'cpu'.\n",
    "        seed : int\n",
    "            random seed. Default: 0.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "        dataset : dic\n",
    "            Train/test inputs/labels are dataset['train_input'], dataset['train_label'],\n",
    "                        dataset['test_input'], dataset['test_label']\n",
    "         \n",
    "    Example\n",
    "    -------\n",
    "    >>> f = lambda x: torch.exp(torch.sin(torch.pi*x[:,[0]]) + x[:,[1]]**2)\n",
    "    >>> dataset = create_dataset(f, n_var=2, train_num=100)\n",
    "    >>> dataset['train_input'].shape\n",
    "    torch.Size([100, 2])\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if len(np.array(ranges).shape) == 1:\n",
    "        ranges = np.array(ranges * n_var).reshape(n_var,2)\n",
    "    else:\n",
    "        ranges = np.array(ranges)\n",
    "        \n",
    "    \n",
    "    train_input = torch.zeros(train_num, n_var)\n",
    "    test_input = torch.zeros(test_num, n_var)\n",
    "    for i in range(n_var):\n",
    "        train_input[:,i] = torch.rand(train_num,)*(ranges[i,1]-ranges[i,0])+ranges[i,0]\n",
    "        test_input[:,i] = torch.rand(test_num,)*(ranges[i,1]-ranges[i,0])+ranges[i,0]\n",
    "                \n",
    "    if f_mode == 'col':\n",
    "        train_label = f(train_input)\n",
    "        test_label = f(test_input)\n",
    "    elif f_mode == 'row':\n",
    "        train_label = f(train_input.T)\n",
    "        test_label = f(test_input.T)\n",
    "    else:\n",
    "        print(f'f_mode {f_mode} not recognized')\n",
    "        \n",
    "    # if has only 1 dimension\n",
    "    if len(train_label.shape) == 1:\n",
    "        train_label = train_label.unsqueeze(dim=1)\n",
    "        test_label = test_label.unsqueeze(dim=1)\n",
    "        \n",
    "    def normalize(data, mean, std):\n",
    "            return (data-mean)/std\n",
    "            \n",
    "    if normalize_input == True:\n",
    "        mean_input = torch.mean(train_input, dim=0, keepdim=True)\n",
    "        std_input = torch.std(train_input, dim=0, keepdim=True)\n",
    "        train_input = normalize(train_input, mean_input, std_input)\n",
    "        test_input = normalize(test_input, mean_input, std_input)\n",
    "        \n",
    "    if normalize_label == True:\n",
    "        mean_label = torch.mean(train_label, dim=0, keepdim=True)\n",
    "        std_label = torch.std(train_label, dim=0, keepdim=True)\n",
    "        train_label = normalize(train_label, mean_label, std_label)\n",
    "        test_label = normalize(test_label, mean_label, std_label)\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train_input'] = train_input.to(device)\n",
    "    dataset['test_input'] = test_input.to(device)\n",
    "\n",
    "    dataset['train_label'] = train_label.to(device)\n",
    "    dataset['test_label'] = test_label.to(device)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultKAN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultKAN(nn.Module):\n",
    "    def __init__(self, width=None, grid=3, k=3, mult_arity = 2, noise_scale=0.3, scale_base_mu=0.0, scale_base_sigma=1.0, base_fun='silu', symbolic_enabled=True, affine_trainable=False, \n",
    "                grid_eps=0.02, grid_range=[-1, 1], sp_trainable=True, sb_trainable=True, seed=1, save_act=True, sparse_init=False, auto_save=True, first_init=True, ckpt_path='./model', \n",
    "                state_id=0, round=0, device='cpu'):\n",
    "\n",
    "        super(MultKAN, self).__init__()\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        ### initializeing the numerical front ###\n",
    "\n",
    "        self.act_fun = []\n",
    "        self.depth = len(width) - 1\n",
    "        \n",
    "        #print('haha1', width)\n",
    "        for i in range(len(width)):\n",
    "            #print(type(width[i]), type(width[i]) == int)\n",
    "            if type(width[i]) == int or type(width[i]) == np.int64:\n",
    "                width[i] = [width[i],0]\n",
    "                \n",
    "        #print('haha2', width)\n",
    "            \n",
    "        self.width = width\n",
    "        \n",
    "        # if mult_arity is just a scalar, we extend it to a list of lists\n",
    "        # e.g, mult_arity = [[2,3],[4]] means that in the first hidden layer, 2 mult ops have arity 2 and 3, respectively;\n",
    "        # in the second hidden layer, 1 mult op has arity 4.\n",
    "        if isinstance(mult_arity, int):\n",
    "            self.mult_homo = True # when homo is True, parallelization is possible\n",
    "        else:\n",
    "            self.mult_homo = False # when home if False, for loop is required. \n",
    "        self.mult_arity = mult_arity\n",
    "\n",
    "        width_in = self.width_in\n",
    "        width_out = self.width_out\n",
    "        \n",
    "        self.base_fun_name = base_fun\n",
    "        if base_fun == 'silu':\n",
    "            base_fun = torch.nn.SiLU()\n",
    "        elif base_fun == 'identity':\n",
    "            base_fun = torch.nn.Identity()\n",
    "        elif base_fun == 'zero':\n",
    "            base_fun = lambda x: x*0.\n",
    "            \n",
    "        self.grid_eps = grid_eps\n",
    "        self.grid_range = grid_range\n",
    "            \n",
    "        \n",
    "        for l in range(self.depth):\n",
    "            # splines\n",
    "            if isinstance(grid, list):\n",
    "                grid_l = grid[l]\n",
    "            else:\n",
    "                grid_l = grid\n",
    "                \n",
    "            if isinstance(k, list):\n",
    "                k_l = k[l]\n",
    "            else:\n",
    "                k_l = k\n",
    "                    \n",
    "            \n",
    "            sp_batch = KANLayer(in_dim=width_in[l], out_dim=width_out[l+1], num=grid_l, k=k_l, noise_scale=noise_scale, scale_base_mu=scale_base_mu, scale_base_sigma=scale_base_sigma, scale_sp=1., base_fun=base_fun, grid_eps=grid_eps, grid_range=grid_range, sp_trainable=sp_trainable, sb_trainable=sb_trainable, sparse_init=sparse_init)\n",
    "            self.act_fun.append(sp_batch)\n",
    "\n",
    "        self.node_bias = []\n",
    "        self.node_scale = []\n",
    "        self.subnode_bias = []\n",
    "        self.subnode_scale = []\n",
    "        \n",
    "        globals()['self.node_bias_0'] = torch.nn.Parameter(torch.zeros(3,1)).requires_grad_(False)\n",
    "        exec('self.node_bias_0' + \" = torch.nn.Parameter(torch.zeros(3,1)).requires_grad_(False)\")\n",
    "        \n",
    "        for l in range(self.depth):\n",
    "            exec(f'self.node_bias_{l} = torch.nn.Parameter(torch.zeros(width_in[l+1])).requires_grad_(affine_trainable)')\n",
    "            exec(f'self.node_scale_{l} = torch.nn.Parameter(torch.ones(width_in[l+1])).requires_grad_(affine_trainable)')\n",
    "            exec(f'self.subnode_bias_{l} = torch.nn.Parameter(torch.zeros(width_out[l+1])).requires_grad_(affine_trainable)')\n",
    "            exec(f'self.subnode_scale_{l} = torch.nn.Parameter(torch.ones(width_out[l+1])).requires_grad_(affine_trainable)')\n",
    "            exec(f'self.node_bias.append(self.node_bias_{l})')\n",
    "            exec(f'self.node_scale.append(self.node_scale_{l})')\n",
    "            exec(f'self.subnode_bias.append(self.subnode_bias_{l})')\n",
    "            exec(f'self.subnode_scale.append(self.subnode_scale_{l})')\n",
    "            \n",
    "        \n",
    "        self.act_fun = nn.ModuleList(self.act_fun)\n",
    "\n",
    "        self.grid = grid\n",
    "        self.k = k\n",
    "        self.base_fun = base_fun\n",
    "\n",
    "        self.affine_trainable = affine_trainable\n",
    "        self.sp_trainable = sp_trainable\n",
    "        self.sb_trainable = sb_trainable\n",
    "        \n",
    "        self.save_act = save_act\n",
    "            \n",
    "        self.node_scores = None\n",
    "        self.edge_scores = None\n",
    "        self.subnode_scores = None\n",
    "        \n",
    "        self.cache_data = None\n",
    "        self.acts = None\n",
    "        \n",
    "        self.auto_save = auto_save\n",
    "        self.state_id = 0\n",
    "        # self.ckpt_path = ckpt_path\n",
    "        self.round = round\n",
    "        \n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "        # if auto_save:\n",
    "        #     if first_init:\n",
    "        #         if not os.path.exists(ckpt_path):\n",
    "        #             # Create the directory\n",
    "        #             os.makedirs(ckpt_path)\n",
    "        #         print(f\"checkpoint directory created: {ckpt_path}\")\n",
    "        #         print('saving model version 0.0')\n",
    "\n",
    "        #         history_path = self.ckpt_path+'/history.txt'\n",
    "        #         with open(history_path, 'w') as file:\n",
    "        #             file.write(f'### Round {self.round} ###' + '\\n')\n",
    "        #             file.write('init => 0.0' + '\\n')\n",
    "        #         self.saveckpt(path=self.ckpt_path+'/'+'0.0')\n",
    "        #     else:\n",
    "        #         self.state_id = state_id\n",
    "            \n",
    "        self.input_id = torch.arange(self.width_in[0],)\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        super(MultKAN, self).to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        for kanlayer in self.act_fun:\n",
    "            kanlayer.to(device)\n",
    "            \n",
    "        # for symbolic_kanlayer in self.symbolic_fun:\n",
    "        #     symbolic_kanlayer.to(device)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def width_in(self):\n",
    "        '''\n",
    "        The number of input nodes for each layer\n",
    "        '''\n",
    "        width = self.width\n",
    "        width_in = [width[l][0]+width[l][1] for l in range(len(width))]\n",
    "        return width_in\n",
    "        \n",
    "    @property\n",
    "    def width_out(self):\n",
    "        '''\n",
    "        The number of output subnodes for each layer\n",
    "        '''\n",
    "        width = self.width\n",
    "        if self.mult_homo == True:\n",
    "            width_out = [width[l][0]+self.mult_arity*width[l][1] for l in range(len(width))]\n",
    "        else:\n",
    "            width_out = [width[l][0]+int(np.sum(self.mult_arity[l])) for l in range(len(width))]\n",
    "        return width_out\n",
    "    \n",
    "    def get_params(self):\n",
    "        '''\n",
    "        Get parameters\n",
    "        '''\n",
    "        return self.parameters()\n",
    "    \n",
    "    def get_act(self, x=None):\n",
    "        '''\n",
    "        collect intermidate activations\n",
    "        '''\n",
    "        if isinstance(x, dict):\n",
    "            x = x['train_input']\n",
    "        if x == None:\n",
    "            if self.cache_data != None:\n",
    "                x = self.cache_data\n",
    "            else:\n",
    "                raise Exception(\"missing input data x\")\n",
    "        save_act = self.save_act\n",
    "        self.save_act = True\n",
    "        self.forward(x)\n",
    "        self.save_act = save_act\n",
    "    \n",
    "    def update_grid_from_samples(self, x):\n",
    "        for l in range(self.depth):\n",
    "            self.get_act(x)\n",
    "            self.act_fun[l].update_grid_from_samples(self.acts[l])\n",
    "            \n",
    "    def update_grid(self, x):\n",
    "        '''\n",
    "        call update_grid_from_samples. This seems unnecessary but we retain it for the sake of classes that might inherit from MultKAN\n",
    "        '''\n",
    "        self.update_grid_from_samples(x)\n",
    "\n",
    "    def forward(self, x, singularity_avoiding=False, y_th=10.):\n",
    "\n",
    "        x = x[:,self.input_id.long()]\n",
    "        assert x.shape[1] == self.width_in[0]\n",
    "        \n",
    "        # cache data\n",
    "        self.cache_data = x\n",
    "        \n",
    "        self.acts = []  # shape ([batch, n0], [batch, n1], ..., [batch, n_L])\n",
    "        self.acts_premult = []\n",
    "        self.spline_preacts = []\n",
    "        self.spline_postsplines = []\n",
    "        self.spline_postacts = []\n",
    "        self.acts_scale = []\n",
    "        self.acts_scale_spline = []\n",
    "        self.subnode_actscale = []\n",
    "        self.edge_actscale = []\n",
    "\n",
    "        self.acts.append(x)  # acts shape: (batch, width[l])\n",
    "\n",
    "        for l in range(self.depth):\n",
    "            \n",
    "            x_numerical, preacts, postacts_numerical, postspline = self.act_fun[l](x)\n",
    "            #print(preacts, postacts_numerical, postspline)\n",
    "            \n",
    "            # if self.symbolic_enabled == True:\n",
    "            #     x_symbolic, postacts_symbolic = self.symbolic_fun[l](x, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "            # else:\n",
    "            x_symbolic = 0.\n",
    "            postacts_symbolic = 0.\n",
    "\n",
    "            x = x_numerical + x_symbolic\n",
    "            \n",
    "            if self.save_act:\n",
    "                # save subnode_scale\n",
    "                self.subnode_actscale.append(torch.std(x, dim=0).detach())\n",
    "            \n",
    "            # subnode affine transform\n",
    "            x = self.subnode_scale[l][None,:] * x + self.subnode_bias[l][None,:]\n",
    "            \n",
    "            if self.save_act:\n",
    "                postacts = postacts_numerical + postacts_symbolic\n",
    "\n",
    "                # self.neurons_scale.append(torch.mean(torch.abs(x), dim=0))\n",
    "                #grid_reshape = self.act_fun[l].grid.reshape(self.width_out[l + 1], self.width_in[l], -1)\n",
    "                input_range = torch.std(preacts, dim=0) + 0.1\n",
    "                output_range_spline = torch.std(postacts_numerical, dim=0) # for training, only penalize the spline part\n",
    "                output_range = torch.std(postacts, dim=0) # for visualization, include the contribution from both spline + symbolic\n",
    "                # save edge_scale\n",
    "                self.edge_actscale.append(output_range)\n",
    "                \n",
    "                self.acts_scale.append((output_range / input_range).detach())\n",
    "                self.acts_scale_spline.append(output_range_spline / input_range)\n",
    "                self.spline_preacts.append(preacts.detach())\n",
    "                self.spline_postacts.append(postacts.detach())\n",
    "                self.spline_postsplines.append(postspline.detach())\n",
    "\n",
    "                self.acts_premult.append(x.detach())\n",
    "            \n",
    "            # multiplication\n",
    "            dim_sum = self.width[l+1][0]\n",
    "            dim_mult = self.width[l+1][1]\n",
    "            \n",
    "            if self.mult_homo == True:\n",
    "                for i in range(self.mult_arity-1):\n",
    "                    if i == 0:\n",
    "                        x_mult = x[:,dim_sum::self.mult_arity] * x[:,dim_sum+1::self.mult_arity]\n",
    "                    else:\n",
    "                        x_mult = x_mult * x[:,dim_sum+i+1::self.mult_arity]\n",
    "                        \n",
    "            else:\n",
    "                for j in range(dim_mult):\n",
    "                    acml_id = dim_sum + np.sum(self.mult_arity[l+1][:j])\n",
    "                    for i in range(self.mult_arity[l+1][j]-1):\n",
    "                        if i == 0:\n",
    "                            x_mult_j = x[:,[acml_id]] * x[:,[acml_id+1]]\n",
    "                        else:\n",
    "                            x_mult_j = x_mult_j * x[:,[acml_id+i+1]]\n",
    "                            \n",
    "                    if j == 0:\n",
    "                        x_mult = x_mult_j\n",
    "                    else:\n",
    "                        x_mult = torch.cat([x_mult, x_mult_j], dim=1)\n",
    "                \n",
    "            if self.width[l+1][1] > 0:\n",
    "                x = torch.cat([x[:,:dim_sum], x_mult], dim=1)\n",
    "            \n",
    "            # x = x + self.biases[l].weight\n",
    "            # node affine transform\n",
    "            x = self.node_scale[l][None,:] * x + self.node_bias[l][None,:]\n",
    "            \n",
    "            self.acts.append(x.detach())\n",
    "    \n",
    "    def fit(self, dataset, opt=\"LBFGS\", steps=100, log=1, lamb=0., lamb_l1=1., lamb_entropy=2., lamb_coef=0., lamb_coefdiff=0., update_grid=True, grid_update_num=10, \n",
    "            loss_fn=None, lr=1.,start_grid_update_step=-1, stop_grid_update_step=50, batch=-1, metrics=None, in_vars=None, out_vars=None, beta=3, \n",
    "            singularity_avoiding=False, y_th=1000., reg_metric='edge_forward_spline_n', display_metrics=None):\n",
    "\n",
    "        if lamb > 0. and not self.save_act:\n",
    "            print('setting lamb=0. If you want to set lamb > 0, set self.save_act=True')\n",
    "            \n",
    "        # old_save_act, old_symbolic_enabled = self.disable_symbolic_in_fit(lamb)\n",
    "\n",
    "        pbar = tqdm(range(steps), desc='description', ncols=100)\n",
    "\n",
    "        # Setting loss function\n",
    "        if loss_fn == None:\n",
    "            loss_fn = loss_fn_eval = lambda x, y: torch.mean((x - y) ** 2)\n",
    "        else:\n",
    "            loss_fn = loss_fn_eval = loss_fn\n",
    "\n",
    "        grid_update_freq = int(stop_grid_update_step / grid_update_num)\n",
    "\n",
    "        if opt == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.get_params(), lr=lr)\n",
    "        elif opt == \"LBFGS\":\n",
    "            optimizer = LBFGS(self.get_params(), lr=lr, history_size=10, line_search_fn=\"strong_wolfe\", tolerance_grad=1e-32, tolerance_change=1e-32, tolerance_ys=1e-32)\n",
    "        \n",
    "\n",
    "        # Setting results dictionary\n",
    "        results = {}\n",
    "        results['train_loss'] = []\n",
    "        results['test_loss'] = []\n",
    "        results['reg'] = []\n",
    "        if metrics != None:\n",
    "            for i in range(len(metrics)):\n",
    "                results[metrics[i].__name__] = []\n",
    "\n",
    "        if batch == -1 or batch > dataset['train_input'].shape[0]:\n",
    "            batch_size = dataset['train_input'].shape[0]\n",
    "            batch_size_test = dataset['test_input'].shape[0]\n",
    "        else:\n",
    "            batch_size = batch\n",
    "            batch_size_test = batch\n",
    "\n",
    "        global train_loss, reg_\n",
    "\n",
    "        # Define closure training functions\n",
    "        def closure():\n",
    "            global train_loss, reg_\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.forward(dataset['train_input'][train_id], singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "            train_loss = loss_fn(pred, dataset['train_label'][train_id])\n",
    "            if self.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    self.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    self.node_attribute()\n",
    "                reg_ = self.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_ = torch.tensor(0.)\n",
    "            objective = train_loss + lamb * reg_\n",
    "            objective.backward()\n",
    "            return objective\n",
    "        \n",
    "        # Setting training loops\n",
    "        for _ in pbar:\n",
    "            \n",
    "            if _ == steps-1 and old_save_act:\n",
    "                self.save_act = True\n",
    "                \n",
    "            # if save_fig and _ % save_fig_freq == 0:\n",
    "            #     save_act = self.save_act\n",
    "            #     self.save_act = True\n",
    "            \n",
    "            train_id = np.random.choice(dataset['train_input'].shape[0], batch_size, replace=False)\n",
    "            test_id = np.random.choice(dataset['test_input'].shape[0], batch_size_test, replace=False)\n",
    "\n",
    "            if _ % grid_update_freq == 0 and _ < stop_grid_update_step and update_grid and _ >= start_grid_update_step:\n",
    "                self.update_grid(dataset['train_input'][train_id])\n",
    "\n",
    "            if opt == \"LBFGS\":\n",
    "                optimizer.step(closure)\n",
    "\n",
    "            if opt == \"Adam\":\n",
    "                pred = self.forward(dataset['train_input'][train_id], singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "                train_loss = loss_fn(pred, dataset['train_label'][train_id])\n",
    "                if self.save_act:\n",
    "                    if reg_metric == 'edge_backward':\n",
    "                        self.attribute()\n",
    "                    if reg_metric == 'node_backward':\n",
    "                        self.node_attribute()\n",
    "                    reg_ = self.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "                else:\n",
    "                    reg_ = torch.tensor(0.)\n",
    "                loss = train_loss + lamb * reg_\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            test_loss = loss_fn_eval(self.forward(dataset['test_input'][test_id]), dataset['test_label'][test_id])\n",
    "            \n",
    "            \n",
    "            if metrics != None:\n",
    "                for i in range(len(metrics)):\n",
    "                    results[metrics[i].__name__].append(metrics[i]().item())\n",
    "\n",
    "            results['train_loss'].append(torch.sqrt(train_loss).cpu().detach().numpy())\n",
    "            results['test_loss'].append(torch.sqrt(test_loss).cpu().detach().numpy())\n",
    "            results['reg'].append(reg_.cpu().detach().numpy())\n",
    "\n",
    "            if _ % log == 0:\n",
    "                if display_metrics == None:\n",
    "                    pbar.set_description(\"| train_loss: %.2e | test_loss: %.2e | reg: %.2e | \" % (torch.sqrt(train_loss).cpu().detach().numpy(), torch.sqrt(test_loss).cpu().detach().numpy(), reg_.cpu().detach().numpy()))\n",
    "                else:\n",
    "                    string = ''\n",
    "                    data = ()\n",
    "                    for metric in display_metrics:\n",
    "                        string += f' {metric}: %.2e |'\n",
    "                        try:\n",
    "                            results[metric]\n",
    "                        except:\n",
    "                            raise Exception(f'{metric} not recognized')\n",
    "                        data += (results[metric][-1],)\n",
    "                    pbar.set_description(string % data)\n",
    "                    \n",
    "            \n",
    "            # if save_fig and _ % save_fig_freq == 0:\n",
    "            #     self.plot(folder=img_folder, in_vars=in_vars, out_vars=out_vars, title=\"Step {}\".format(_), beta=beta)\n",
    "            #     plt.savefig(img_folder + '/' + str(_) + '.jpg', bbox_inches='tight', dpi=200)\n",
    "            #     plt.close()\n",
    "            #     self.save_act = save_act\n",
    "\n",
    "        self.log_history('fit')\n",
    "        # revert back to original state\n",
    "        self.symbolic_enabled = old_symbolic_enabled\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "description:   0%|                                                           | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39msin(torch\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39mx[:,[\u001b[38;5;241m0\u001b[39m]]) \u001b[38;5;241m+\u001b[39m x[:,[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m create_dataset(f, n_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 373\u001b[0m, in \u001b[0;36mMultKAN.fit\u001b[0;34m(self, dataset, opt, steps, log, lamb, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff, update_grid, grid_update_num, loss_fn, lr, start_grid_update_step, stop_grid_update_step, batch, metrics, in_vars, out_vars, beta, singularity_avoiding, y_th, reg_metric, display_metrics)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    372\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_input\u001b[39m\u001b[38;5;124m'\u001b[39m][train_id], singularity_avoiding\u001b[38;5;241m=\u001b[39msingularity_avoiding, y_th\u001b[38;5;241m=\u001b[39my_th)\n\u001b[0;32m--> 373\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_act:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m reg_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_backward\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[40], line 304\u001b[0m, in \u001b[0;36mMultKAN.fit.<locals>.<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Setting loss function\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_fn \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m loss_fn_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, y: torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m loss_fn_eval \u001b[38;5;241m=\u001b[39m loss_fn\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "model = MultKAN(width=[2,5,1], grid=5, k=3, noise_scale=0.3, seed=2)\n",
    "f = lambda x: torch.exp(torch.sin(torch.pi*x[:,[0]]) + x[:,[1]]**2)\n",
    "dataset = create_dataset(f, n_var=2)\n",
    "model.fit(dataset, opt='Adam', steps=20, lamb=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
